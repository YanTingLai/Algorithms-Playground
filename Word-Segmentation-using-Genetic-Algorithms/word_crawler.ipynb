{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "board_list = [\n",
    "    'NBA', 'Stock', 'Lifeismoney',\n",
    "    'Baseball', 'movie', 'LoL', 'car', \n",
    "    'MobileComm', 'marvel', 'PC_Shopping', 'Tech_Job', 'Salary', \n",
    "    'StupidClown', 'CVS', 'Soft_Job', 'Bank_Service', 'Finance'\n",
    "]\n",
    "\n",
    "# 特殊符號分割\n",
    "pattern = r'\\n| |\\*|？|　|：|,|\\.|/|;|\\'|`|\\[|\\]|<|>|\\?|:|\"|\\{|\\}|\\~|!|@|#|\\$|%|\\^|&|\\(|\\)|-|=|\\_|\\+|，|。|、|；|‘|’|【|】|·|！| |…|（|）|〔|／|〕|「|」'\n",
    "\n",
    "user_dict = {}\n",
    "crawl_pages = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeg(text):\n",
    "    if not text:\n",
    "        return ''\n",
    "    if len(text) == 1:\n",
    "        return text\n",
    "    if text in word_dict:\n",
    "        return text\n",
    "    else:\n",
    "        small = len(text) - 1\n",
    "        text = text[0:small]\n",
    "        return getSeg(text)\n",
    "\n",
    "def ptt_crawler(n_page):\n",
    "    if not os.path.exists('source/ptt/'):\n",
    "        os.mkdir('source/ptt/')\n",
    "    target_content = u'※ 發信站: 批踢踢實業坊(ptt.cc),'\n",
    "    for bb in board_list:\n",
    "        url = \"https://www.ptt.cc/bbs/\" + bb + \"/index.html\"\n",
    "        file_name = \"source/ptt/\" + bb + \".txt\"\n",
    "        print('-'*20, bb, '-'*20)\n",
    "        \n",
    "        for i in range(n_page): \n",
    "            r = requests.get(url, verify=False)\n",
    "            r.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "            sel = soup.select(\"div.title a\") #標題\n",
    "            u = soup.select(\"div.btn-group.btn-group-paging a\") #a標籤\n",
    "            print('Starting crawler',url)\n",
    "            for s in sel: \n",
    "                text_url = 'https://www.ptt.cc' + s[\"href\"]\n",
    "                response = requests.get(text_url, verify=False)\n",
    "                response.encoding = 'utf-8'\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "                try:\n",
    "                    content = soup.find(id=\"main-content\").text\n",
    "                    content = content.replace('\\n', '').split('標題')[1].split(target_content)[0].replace('--', '')     \n",
    "                    push_content = soup.find_all(\"span\", {\"class\": \"f3 push-content\"})\n",
    "                    with open(file_name, \"a\") as text_file:\n",
    "                        try:\n",
    "                            text_file.write(content)\n",
    "                            for x in [x.text.replace(':', '') for x in push_content]:\n",
    "                                text_file.write(x)\n",
    "                            text_file.close()\n",
    "                        except:\n",
    "                            pass  \n",
    "                except:\n",
    "                    continue\n",
    "            # 如果本頁是第一頁，則跳出迴圈。\n",
    "            if 'index1.html' in url:\n",
    "                break\n",
    "            else:\n",
    "                # url更新為上一頁網址\n",
    "                url = \"https://www.ptt.cc\"+ u[1][\"href\"] \n",
    "                time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "PTT爬蟲<br>\n",
    "預設爬10頁<br>\n",
    "如果進頁面需要確認滿18歲則會Error<br>\n",
    "後續再調整:O<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- NBA --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/NBA/index.html\n",
      "-------------------- Stock --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/Stock/index.html\n",
      "-------------------- Lifeismoney --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/Lifeismoney/index.html\n",
      "-------------------- Baseball --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/Baseball/index.html\n",
      "-------------------- movie --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/movie/index.html\n",
      "-------------------- LoL --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/LoL/index.html\n",
      "-------------------- car --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/car/index.html\n",
      "-------------------- MobileComm --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/MobileComm/index.html\n",
      "-------------------- marvel --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/marvel/index.html\n",
      "-------------------- PC_Shopping --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/PC_Shopping/index.html\n",
      "-------------------- Tech_Job --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/Tech_Job/index.html\n",
      "-------------------- Salary --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/Salary/index.html\n",
      "-------------------- StupidClown --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/StupidClown/index.html\n",
      "-------------------- CVS --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/CVS/index.html\n",
      "-------------------- Soft_Job --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/Soft_Job/index.html\n",
      "-------------------- Bank_Service --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/Bank_Service/index.html\n",
      "-------------------- Finance --------------------\n",
      "Starting crawler https://www.ptt.cc/bbs/Finance/index.html\n"
     ]
    }
   ],
   "source": [
    "ptt_crawler(crawl_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ptt']\n",
      "['source/ptt/Bank_Service.txt', 'source/ptt/Baseball.txt', 'source/ptt/car.txt', 'source/ptt/CVS.txt', 'source/ptt/Finance.txt', 'source/ptt/Lifeismoney.txt', 'source/ptt/LoL.txt', 'source/ptt/marvel.txt', 'source/ptt/MobileComm.txt', 'source/ptt/movie.txt', 'source/ptt/NBA.txt', 'source/ptt/PC_Shopping.txt', 'source/ptt/Salary.txt', 'source/ptt/Soft_Job.txt', 'source/ptt/Stock.txt', 'source/ptt/StupidClown.txt', 'source/ptt/Tech_Job.txt']\n"
     ]
    }
   ],
   "source": [
    "source_dir = os.listdir('source')\n",
    "for src_cate in source_dir:\n",
    "    filedir = ['source/' + src_cate + '/' + x for x in os.listdir('source/'+src_cate)]\n",
    "\n",
    "print(source_dir)\n",
    "print(filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source/ptt/Bank_Service.txt --------------------\n",
      "source/ptt/Baseball.txt --------------------\n",
      "source/ptt/car.txt --------------------\n",
      "source/ptt/CVS.txt --------------------\n",
      "source/ptt/Finance.txt --------------------\n",
      "source/ptt/Lifeismoney.txt --------------------\n",
      "source/ptt/LoL.txt --------------------\n",
      "source/ptt/marvel.txt --------------------\n",
      "source/ptt/MobileComm.txt --------------------\n",
      "source/ptt/movie.txt --------------------\n",
      "source/ptt/NBA.txt --------------------\n",
      "source/ptt/PC_Shopping.txt --------------------\n",
      "source/ptt/Salary.txt --------------------\n",
      "source/ptt/Soft_Job.txt --------------------\n",
      "source/ptt/Stock.txt --------------------\n",
      "source/ptt/StupidClown.txt --------------------\n",
      "source/ptt/Tech_Job.txt --------------------\n"
     ]
    }
   ],
   "source": [
    "for fn in filedir:\n",
    "    print(fn, '-'*20)\n",
    "    ff = open(fn, 'r').readlines()\n",
    "    for string_raw in ff:\n",
    "        result_list = re.split(pattern, string_raw)\n",
    "        for string in result_list:\n",
    "            string = string.replace('\\n', '')\n",
    "            for n_ in list(range(len(string))):\n",
    "                for n_len in list(range(n_, len(string))):\n",
    "                    nn = string[n_:n_len+1]\n",
    "                    if nn in user_dict.keys():\n",
    "                        user_dict[nn] += 1\n",
    "                    else:\n",
    "                        user_dict[nn] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary\n",
    "json_text = json.dumps(user_dict)\n",
    "f = open(\"dict.json\",\"w\")\n",
    "f.write(json_text)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整理後存成txt\n",
    "pd_dict = pd.DataFrame.from_dict(user_dict, orient='index')\n",
    "pd_dict.reset_index(inplace=True)\n",
    "\n",
    "pd_dict.columns = ['word', 'count']\n",
    "pd_dict = pd_dict[pd_dict['count'] > 10]\n",
    "pd_dict = pd_dict[pd_dict['word'].str.len() > 1]\n",
    "pd_dict = pd_dict[pd_dict['word'].str.len() < 9]\n",
    "pd_dict['count_log'] = np.log(pd_dict['count'])\n",
    "pd_dict['word_len'] = pd_dict['word'].str.len()\n",
    "pd_dict['count_multiply_word_len'] = pd_dict['count'] * pd_dict['word_len']\n",
    "pd_dict['count_multiply_word_len_square'] = pd_dict['count'] * pd_dict['word_len'] * pd_dict['word_len']\n",
    "pd_dict['count_log_multiply_word_len'] = pd_dict['count_log'] * pd_dict['word_len']\n",
    "pd_dict['count_log_multiply_word_len_square'] = pd_dict['count_log'] * pd_dict['word_len'] * pd_dict['word_len']\n",
    "\n",
    "pd_dict.to_csv('word.txt', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
